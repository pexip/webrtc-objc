From cb7f6c5e2f11c58b2c266fee1f8ba66271c4bf2f Mon Sep 17 00:00:00 2001
From: Vadym Markov <markov.vadym@gmail.com>
Date: Mon, 28 Mar 2022 23:00:41 +0200
Subject: [PATCH] Apply macCatalyst fixes

---
 BUILD.gn                                      |  4 +++
 sdk/BUILD.gn                                  | 16 ++++++++--
 .../capturer/RTCCameraVideoCapturer.m         | 11 ++++++-
 .../RTCDefaultVideoDecoderFactory.m           | 14 +++++++++
 .../RTCDefaultVideoEncoderFactory.m           | 14 +++++++++
 sdk/objc/native/src/audio/audio_device_ios.h  |  3 ++
 sdk/objc/native/src/audio/audio_device_ios.mm | 29 +++++++++++++++++++
 .../src/audio/voice_processing_audio_unit.mm  | 12 ++++++--
 8 files changed, 97 insertions(+), 6 deletions(-)

diff --git a/BUILD.gn b/BUILD.gn
index fa4fcace49..02ac87033a 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -265,6 +265,10 @@ config("common_config") {
     defines += [ "WEBRTC_INCLUDE_INTERNAL_AUDIO_DEVICE" ]
   }
 
+  if (rtc_build_libvpx) {
+    defines += [ "RTC_ENABLE_VP8" ]
+  }
+
   if (rtc_libvpx_build_vp9) {
     defines += [ "RTC_ENABLE_VP9" ]
   }
diff --git a/sdk/BUILD.gn b/sdk/BUILD.gn
index 718d9274a1..27fe296a27 100644
--- a/sdk/BUILD.gn
+++ b/sdk/BUILD.gn
@@ -661,10 +661,18 @@ if (is_ios || is_mac) {
         ":native_video",
         ":videocodec_objc",
         ":videotoolbox_objc",
-        ":vp8",
-        ":vp9",
-        ":vpx_codec_constants",
       ]
+
+      if (rtc_build_libvpx) {
+        deps += [
+          ":vp8",
+          ":vpx_codec_constants",
+        ]
+      }
+
+      if (rtc_libvpx_build_vp9) {
+        deps += [ ":vp9" ]
+      }
     }
 
     rtc_library("vpx_codec_constants") {
@@ -1335,6 +1343,8 @@ if (is_ios || is_mac) {
         ]
         if (rtc_ios_macos_use_opengl_rendering) {
           deps += [ ":opengl_ui_objc" ]
+        } else {
+          deps += [ ":metal_objc" ]    
         }
         if (!build_with_chromium) {
           deps += [
diff --git a/sdk/objc/components/capturer/RTCCameraVideoCapturer.m b/sdk/objc/components/capturer/RTCCameraVideoCapturer.m
index e0e9e41254..cef36fb1b4 100644
--- a/sdk/objc/components/capturer/RTCCameraVideoCapturer.m
+++ b/sdk/objc/components/capturer/RTCCameraVideoCapturer.m
@@ -74,9 +74,18 @@ - (instancetype)initWithDelegate:(__weak id<RTC_OBJC_TYPE(RTCVideoCapturerDelega
       return nil;
     }
     NSNotificationCenter *center = [NSNotificationCenter defaultCenter];
+
 #if TARGET_OS_IPHONE
+#if TARGET_OS_MACCATALYST
+    _orientation = UIDeviceOrientationLandscapeRight;
+    _rotation = RTCVideoRotation_0;
+#else
     _orientation = UIDeviceOrientationPortrait;
     _rotation = RTCVideoRotation_90;
+#endif
+#endif
+
+#if TARGET_OS_IPHONE
     [center addObserver:self
                selector:@selector(deviceOrientationDidChange:)
                    name:UIDeviceOrientationDidChangeNotification
@@ -520,7 +529,7 @@ - (void)reconfigureCaptureSessionInput {
 - (void)updateOrientation {
   NSAssert([RTC_OBJC_TYPE(RTCDispatcher) isOnQueueForType:RTCDispatcherTypeCaptureSession],
            @"updateOrientation must be called on the capture queue.");
-#if TARGET_OS_IPHONE
+#if TARGET_OS_IPHONE && !TARGET_OS_MACCATALYST
   _orientation = [UIDevice currentDevice].orientation;
 #endif
 }
diff --git a/sdk/objc/components/video_codec/RTCDefaultVideoDecoderFactory.m b/sdk/objc/components/video_codec/RTCDefaultVideoDecoderFactory.m
index f4a97a8659..ccb816060d 100644
--- a/sdk/objc/components/video_codec/RTCDefaultVideoDecoderFactory.m
+++ b/sdk/objc/components/video_codec/RTCDefaultVideoDecoderFactory.m
@@ -14,8 +14,12 @@
 #import "RTCVideoDecoderH264.h"
 #import "api/video_codec/RTCVideoCodecConstants.h"
 #import "api/video_codec/RTCVideoDecoderAV1.h"
+#if defined(RTC_ENABLE_VP8)
 #import "api/video_codec/RTCVideoDecoderVP8.h"
+#endif
+#if defined(RTC_ENABLE_VP9)
 #import "api/video_codec/RTCVideoDecoderVP9.h"
+#endif
 #import "base/RTCVideoCodecInfo.h"
 
 @implementation RTC_OBJC_TYPE (RTCDefaultVideoDecoderFactory)
@@ -39,19 +43,25 @@ @implementation RTC_OBJC_TYPE (RTCDefaultVideoDecoderFactory)
       [[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecH264Name
                                                   parameters:constrainedBaselineParams];
 
+  #if defined(RTC_ENABLE_VP8)
   RTC_OBJC_TYPE(RTCVideoCodecInfo) *vp8Info =
       [[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecVp8Name];
+  #endif
 
   NSMutableArray<RTC_OBJC_TYPE(RTCVideoCodecInfo) *> *result = [@[
     constrainedHighInfo,
     constrainedBaselineInfo,
+    #if defined(RTC_ENABLE_VP8)
     vp8Info,
+    #endif
   ] mutableCopy];
 
+  #if defined(RTC_ENABLE_VP9)
   if ([RTC_OBJC_TYPE(RTCVideoDecoderVP9) isSupported]) {
     [result
         addObject:[[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecVp9Name]];
   }
+  #endif
 
   if ([RTC_OBJC_TYPE(RTCVideoDecoderAV1) isSupported]) {
     [result
@@ -64,11 +74,15 @@ @implementation RTC_OBJC_TYPE (RTCDefaultVideoDecoderFactory)
 - (id<RTC_OBJC_TYPE(RTCVideoDecoder)>)createDecoder:(RTC_OBJC_TYPE(RTCVideoCodecInfo) *)info {
   if ([info.name isEqualToString:kRTCVideoCodecH264Name]) {
     return [[RTC_OBJC_TYPE(RTCVideoDecoderH264) alloc] init];
+  #if defined(RTC_ENABLE_VP8)
   } else if ([info.name isEqualToString:kRTCVideoCodecVp8Name]) {
     return [RTC_OBJC_TYPE(RTCVideoDecoderVP8) vp8Decoder];
+  #endif
+  #if defined(RTC_ENABLE_VP9)
   } else if ([info.name isEqualToString:kRTCVideoCodecVp9Name] &&
              [RTC_OBJC_TYPE(RTCVideoDecoderVP9) isSupported]) {
     return [RTC_OBJC_TYPE(RTCVideoDecoderVP9) vp9Decoder];
+  #endif
   } else if ([info.name isEqualToString:kRTCVideoCodecAv1Name] &&
              [RTC_OBJC_TYPE(RTCVideoDecoderAV1) isSupported]) {
     return [RTC_OBJC_TYPE(RTCVideoDecoderAV1) av1Decoder];
diff --git a/sdk/objc/components/video_codec/RTCDefaultVideoEncoderFactory.m b/sdk/objc/components/video_codec/RTCDefaultVideoEncoderFactory.m
index 06c4e8c22f..1ce818ae6c 100644
--- a/sdk/objc/components/video_codec/RTCDefaultVideoEncoderFactory.m
+++ b/sdk/objc/components/video_codec/RTCDefaultVideoEncoderFactory.m
@@ -14,8 +14,12 @@
 #import "RTCVideoEncoderH264.h"
 #import "api/video_codec/RTCVideoCodecConstants.h"
 #import "api/video_codec/RTCVideoEncoderAV1.h"
+#if defined(RTC_ENABLE_VP8)
 #import "api/video_codec/RTCVideoEncoderVP8.h"
+#endif
+#if defined(RTC_ENABLE_VP9)
 #import "api/video_codec/RTCVideoEncoderVP9.h"
+#endif
 #import "base/RTCVideoCodecInfo.h"
 
 @implementation RTC_OBJC_TYPE (RTCDefaultVideoEncoderFactory)
@@ -41,19 +45,25 @@ @implementation RTC_OBJC_TYPE (RTCDefaultVideoEncoderFactory)
       [[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecH264Name
                                                   parameters:constrainedBaselineParams];
 
+  #if defined(RTC_ENABLE_VP8)
   RTC_OBJC_TYPE(RTCVideoCodecInfo) *vp8Info =
       [[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecVp8Name];
+  #endif
 
   NSMutableArray<RTC_OBJC_TYPE(RTCVideoCodecInfo) *> *result = [@[
     constrainedHighInfo,
     constrainedBaselineInfo,
+    #if defined(RTC_ENABLE_VP8)
     vp8Info,
+    #endif
   ] mutableCopy];
 
+  #if defined(RTC_ENABLE_VP9)
   if ([RTC_OBJC_TYPE(RTCVideoEncoderVP9) isSupported]) {
     [result
         addObject:[[RTC_OBJC_TYPE(RTCVideoCodecInfo) alloc] initWithName:kRTCVideoCodecVp9Name]];
   }
+  #endif
 
   if ([RTC_OBJC_TYPE(RTCVideoEncoderAV1) isSupported]) {
     [result
@@ -66,11 +76,15 @@ @implementation RTC_OBJC_TYPE (RTCDefaultVideoEncoderFactory)
 - (id<RTC_OBJC_TYPE(RTCVideoEncoder)>)createEncoder:(RTC_OBJC_TYPE(RTCVideoCodecInfo) *)info {
   if ([info.name isEqualToString:kRTCVideoCodecH264Name]) {
     return [[RTC_OBJC_TYPE(RTCVideoEncoderH264) alloc] initWithCodecInfo:info];
+  #if defined(RTC_ENABLE_VP8)
   } else if ([info.name isEqualToString:kRTCVideoCodecVp8Name]) {
     return [RTC_OBJC_TYPE(RTCVideoEncoderVP8) vp8Encoder];
+  #endif
+  #if defined(RTC_ENABLE_VP9)
   } else if ([info.name isEqualToString:kRTCVideoCodecVp9Name] &&
              [RTC_OBJC_TYPE(RTCVideoEncoderVP9) isSupported]) {
     return [RTC_OBJC_TYPE(RTCVideoEncoderVP9) vp9Encoder];
+  #endif
   } else if ([info.name isEqualToString:kRTCVideoCodecAv1Name] &&
              [RTC_OBJC_TYPE(RTCVideoEncoderAV1) isSupported]) {
     return [RTC_OBJC_TYPE(RTCVideoEncoderAV1) av1Encoder];
diff --git a/sdk/objc/native/src/audio/audio_device_ios.h b/sdk/objc/native/src/audio/audio_device_ios.h
index 5afc49a461..923418166d 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_ios.h
@@ -264,6 +264,9 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   // simulators, the size can vary from callback to callback and the size
   // will be changed dynamically to account for this behavior.
   rtc::BufferT<int16_t> record_audio_buffer_;
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+  rtc::BufferT<Float32> record_audio_buffer_f32_;
+#endif
 
   // Set to 1 when recording is active and 0 otherwise.
   volatile int recording_;
diff --git a/sdk/objc/native/src/audio/audio_device_ios.mm b/sdk/objc/native/src/audio/audio_device_ios.mm
index 248f3fcfd2..7393aff4cd 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_ios.mm
@@ -389,6 +389,11 @@ static void LogDeviceInfo() {
   record_audio_buffer_.Clear();
   record_audio_buffer_.SetSize(num_frames);
 
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+  record_audio_buffer_f32_.Clear();
+  record_audio_buffer_f32_.SetSize(num_frames);
+#endif
+
   // Allocate AudioBuffers to be used as storage for the received audio.
   // The AudioBufferList structure works as a placeholder for the
   // AudioBuffer structure, which holds a pointer to the actual data buffer
@@ -400,7 +405,11 @@ static void LogDeviceInfo() {
   audio_buffer->mNumberChannels = record_parameters_.channels();
   audio_buffer->mDataByteSize =
       record_audio_buffer_.size() * VoiceProcessingAudioUnit::kBytesPerSample;
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+  audio_buffer->mData = reinterpret_cast<int8_t*>(record_audio_buffer_f32_.data());
+#else
   audio_buffer->mData = reinterpret_cast<int8_t*>(record_audio_buffer_.data());
+#endif
 
   // Obtain the recorded audio samples by initiating a rendering cycle.
   // Since it happens on the input bus, the `io_data` parameter is a reference
@@ -414,6 +423,14 @@ static void LogDeviceInfo() {
     return result;
   }
 
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+  for (UInt32 i = 0; i < num_frames; i++) {
+    Float32 sample = record_audio_buffer_f32_[i];
+    int16_t short_sample = (int16_t)(sample * INT16_MAX);
+    record_audio_buffer_[i] = short_sample;
+  }
+ #endif
+
   // Get a pointer to the recorded audio and send it to the WebRTC ADB.
   // Use the FineAudioBuffer instance to convert between native buffer size
   // and the 10ms buffer size used by WebRTC.
@@ -477,6 +494,18 @@ static void LogDeviceInfo() {
   fine_audio_buffer_->GetPlayoutData(
       rtc::ArrayView<int16_t>(static_cast<int16_t*>(audio_buffer->mData), num_frames),
       kFixedPlayoutDelayEstimate);
+
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+  int16_t* short_data = static_cast<int16_t*>(audio_buffer->mData);
+  Float32* data = static_cast<Float32*>(audio_buffer->mData);
+  for (UInt32 i = 0; i < num_frames; i++) {
+    UInt32 j = num_frames - i - 1;
+    int16_t short_sample = short_data[j];
+    Float32 sample = (Float32)short_sample / INT16_MAX;
+    data[j] = sample;
+  }
+#endif
+  
   return noErr;
 }
 
diff --git a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
index 3905b6857a..3a716d81e6 100644
--- a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
+++ b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
@@ -84,7 +84,11 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   DisposeAudioUnit();
 }
 
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+const UInt32 VoiceProcessingAudioUnit::kBytesPerSample = 4;
+#else
 const UInt32 VoiceProcessingAudioUnit::kBytesPerSample = 2;
+#endif
 
 bool VoiceProcessingAudioUnit::Init() {
   RTC_DCHECK_EQ(state_, kInitRequired);
@@ -450,8 +454,12 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   RTC_DCHECK_EQ(1, kRTCAudioSessionPreferredNumberOfChannels);
   format.mSampleRate = sample_rate;
   format.mFormatID = kAudioFormatLinearPCM;
-  format.mFormatFlags =
-      kLinearPCMFormatFlagIsSignedInteger | kLinearPCMFormatFlagIsPacked;
+  format.mFormatFlags = kLinearPCMFormatFlagIsPacked;
+#if defined(WEBRTC_IOS) && TARGET_OS_MACCATALYST
+   format.mFormatFlags |= kLinearPCMFormatFlagIsFloat;
+#else
+   format.mFormatFlags |= kLinearPCMFormatFlagIsSignedInteger;
+#endif
   format.mBytesPerPacket = kBytesPerSample;
   format.mFramesPerPacket = 1;  // uncompressed.
   format.mBytesPerFrame = kBytesPerSample;
-- 
2.32.0 (Apple Git-132)

